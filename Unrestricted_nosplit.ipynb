{"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"aPlF-oZw9jfj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Init"],"metadata":{"id":"DJhbt5o--pQ8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xf2gqVLEGpS7"},"outputs":[],"source":["!pip install transformers\n","\n","!pip install datasets\n","\n","!pip install sacrebleu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zL03b1spGz8Z"},"outputs":[],"source":["!pip install torch==1.11.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIMccdLgYisZ"},"outputs":[],"source":["import random\n","random.seed(100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pr-Iv48fG2da"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xq3G35neG3yj"},"outputs":[],"source":["# drive.mount(\"/content/gdrive\", force_remount=True)"]},{"cell_type":"markdown","source":["## Load Datasets"],"metadata":{"id":"L-dgcONc-w9a"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_uqBSrD7G6Hx"},"outputs":[],"source":["BASE_PATH = \"/content/gdrive/MyDrive/experiments\"\n","TRAIN_UNRESTRICTED_PATH = f\"{BASE_PATH}/unrestricted_train_dataset-140.csv\"\n","OOV_UNRESTRICTED_PATH = f\"{BASE_PATH}/unrestricted_test_dataset.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjKGJF_AO6qB"},"outputs":[],"source":["from datasets import load_dataset, Dataset, DatasetDict\n","\n","unrestricted_train = load_dataset('csv', data_files=TRAIN_UNRESTRICTED_PATH)\n","unrestricted_test = load_dataset('csv', data_files=OOV_UNRESTRICTED_PATH)\n","\n","dataset = DatasetDict({'train': unrestricted_train['train'],'test': unrestricted_test['train']})\n","\n","dataset"]},{"cell_type":"markdown","source":["## Preprocessing"],"metadata":{"id":"vX-UwSQ2-zZu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Pn4rHCOPBzQ"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJiplxdoPEL6"},"outputs":[],"source":["source_lang = \"en\"\n","target_lang = \"ltl\"\n","prefix = \"translate English to LTL: \"\n","\n","\n","def preprocess_function(examples):\n","    inputs = [prefix + example.replace(',', ' ,') for example in examples[source_lang]]\n","    targets = [example for example in examples[target_lang]]\n","    model_inputs = tokenizer(inputs, max_length=256, truncation=True)\n","\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(targets, max_length=256, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x005cAacPh1v"},"outputs":[],"source":["tokenized_dataset = dataset.map(preprocess_function, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VF6fQbMrPmpf"},"outputs":[],"source":["item = tokenized_dataset['train']['ltl'][0]\n","decoded_item = tokenizer.decode(tokenized_dataset['train']['labels'][0])\n","\n","reversed_vocab = {i: w for w, i in tokenizer.get_vocab().items()}\n","\n","print(item)\n","print(decoded_item)\n","print(len(item.split(' ')), len(tokenized_dataset['train']['labels'][0]), len(decoded_item.split(' ')))\n","print(tokenized_dataset['train']['labels'][0])\n","print([reversed_vocab[i] for i in tokenized_dataset['train']['labels'][0]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZkcXLtpZPnX4"},"outputs":[],"source":["tokenized_dataset"]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"tgGI1bYJ-2lm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pzY3oQ6yPqfp"},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n","\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_usemIGePr5i"},"outputs":[],"source":["print(\"For T5:\")\n","print(\"Tokenizer vocab_size: {}\".format(tokenizer.vocab_size))\n","print(\"Model vocab size: {}\\n\".format(model.config.vocab_size))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXjaVO5nP2wv"},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5VaKZpV4P7yY"},"outputs":[],"source":["RESULTS_DIR = \"/content/gdrive/MyDrive/experiments/unrestricted_nosplit\"\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=RESULTS_DIR,\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=0.0001,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    weight_decay=0.01,\n","    save_total_limit=1,\n","    num_train_epochs=1,\n","    adam_beta2=0.98,\n","    warmup_steps=4000,\n","    optim=\"adamw_torch\",\n","    fp16=True,\n","    predict_with_generate=True,\n","    generation_max_length=256,\n","    save_strategy='epoch'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIwM9IOFP9Q7"},"outputs":[],"source":["import numpy as np\n","from datasets import load_metric\n","\n","metric = load_metric(\"sacrebleu\")\n","\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","    return preds, labels\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    # Some simple post-processing\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    result = {\"bleu\": result[\"score\"]}\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    result = {k: round(v, 4) for k, v in result.items()}\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nu2BO7knP-05"},"outputs":[],"source":["trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# trainer.train(f\"{RESULTS_DIR}/checkpoint-8750/\") # If u want to resume a checkpoint then use this\n","trainer.train()"]},{"cell_type":"markdown","source":["## Evaluate"],"metadata":{"id":"lcQjLe_B_hIZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDMskwb3QBzS"},"outputs":[],"source":["\n","trainer.evaluate(tokenized_dataset[\"test\"], metric_key_prefix='', max_length=256) # the trainer already perform the test, use this if u want to test smth else"]},{"cell_type":"markdown","source":["## Manual Translation"],"metadata":{"id":"Fri52dW1AFv5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fq04hmxEqP0f"},"outputs":[],"source":["from transformers import T5ForConditionalGeneration\n","\n","prediction_model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/gdrive/MyDrive/new_ltl_dataset_generator/first_dataset/checkpoint-6250/\")\n","\n","# model.to('cpu')\n","\n","src_text = tokenized_dataset['test']['en'] # \"translate English to LTL: \" + tokenized_dataset['oov']['en'][0]\n","tokens = tokenizer(src_text, return_tensors=\"pt\", padding=True)\n","# print(tokenizer([tokenized_dataset['oov']['en'][0]], return_tensors=\"pt\", padding=True))\n","print(src_text)\n","print(tokens)\n","\n","# tok_src = {k: v.to('cuda') for k, v in tokenizer(tokenized_dataset['oov']['en'], return_tensors=\"pt\", padding=True).items()}\n","\n","# translated = [prediction_model.generate(**tokenizer([t], return_tensors=\"pt\", padding=True), max_length=500) for t in tokenized_dataset['oov']['en']]\n","translated = prediction_model.generate(**tokens, max_length=256)\n","\n","# [tokenizer.decode(t, skip_special_tokens=True) for t in translated.tolist()]\n","# translated.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"083Mvw5lXH1A"},"outputs":[],"source":["print(type(translated))\n","\n","outs_srcs = [tokenizer.decode(t, skip_special_tokens=True) for t in tokenized_dataset['test']['input_ids']]\n","# outs_lbls = [tokenizer.decode(t, skip_special_tokens=True) for t in translated.predictions]\n","outs_lbls = [tokenizer.decode(t, skip_special_tokens=True) for t in translated.tolist()]\n","\n","outs_srcs[:2], outs_lbls[:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTZNUjsFXLyW"},"outputs":[],"source":["print(tokenized_dataset['test']['ltl'][:10])\n","print(outs_lbls[:10])\n","\n","print(type(tokenized_dataset['test']['ltl']))\n","print(type(outs_lbls))\n","\n","metric.compute(references=[[t] for t in tokenized_dataset['test']['ltl'][:100]], predictions=outs_lbls)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["L-dgcONc-w9a","vX-UwSQ2-zZu","Fri52dW1AFv5"],"authorship_tag":"ABX9TyOVE+rF3xQjcWwwWMJi4qby"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}